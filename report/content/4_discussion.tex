\section{Discussion on the original paper}
\label{sec:discussion}
ADOP\cite{Aruckert2022adop} is overall an excellent paper. It does not bring so much novel ideas but makes a considerable engineering effort to apply the core idea of NPBG \cite{Aliev2020} to large real scenes.
In section ~\ref{subsec:improvement_ideas}, I will discuss some improvement ideas I had while working on the re-implementation of the ADOP paper. But we'll discuss a point of criticism in the next section.\\
\noindent \textbf{~\ref{subsec:limits_real_scenes}{: Real scenes $\approx$ biased evaluation}}. Evaluation on real scenes from the Tanks and Temple dataset \cite{Knapitsch2017TanksAndTemples} mixes two things:
\begin{itemize}
    \item the inherent neural rendering method (point based rendering) quality assesment.
    \item all improvements made by taking into account the camera pipeline.
\end{itemize}
Despite ablation studies to see the effect taking the camera photo pipline into account, the comparison to other methods is unfair and I'll propose a few ideas for a new benchmark to evaluate neural rendering quality in a more controlled environment for research purpose.

\subsection{Limitation: Benchmarking only on real scenes}
\label{subsec:limits_real_scenes}
\noindent \textbf{Focusing only on real scenes.}
Suprisingly, the authors do not mention any attempts to apply their method to synthetic scenes, like the NERF paper initially did and they mention that training a new scene requires a large amount of compute. This could mean they developped their method progressively on a few toy examples and didn't bother releasing results of these tiny examples. Or their method may simply not perform too well on synthetic scenes... which may include a lot of specular materials \footnote{Classical NERF test samples scenes usually contain a lot of specular materials where the rendering would most probably fail}. I still think it would be beneficial to have a few synthetic scenes to test on including to make quick tests without the need of A100 40Gb trained for several hours.
On the other hand, since the main contribution are refining pose estimation (requiring the approximate differentiable aspect of the renderer), handling large scenes and their camera simulator, it's not surprising that the authors showcase their work on natural photos rather than simulation. 

\noindent \textbf{Camera pipeline module.}
The attempt of the authors to model the camera pipeline comes from a good intention. By including the camera pipeline as part of their rendering, they are able to: 

\noindent - work in a linear color space where... performing additions or convolutions is physically grounded.

\noindent - compensate exposure and colors shifts per scene.

\noindent - get a better accuracy on the Tanks and Temple benchmark which has a lot of exposure changes.

\noindent I think the authors picked the low hanging fruits here: take care of most exposure changes to end up improving the metrics on the available benchmark: Table 4. from their paper shows that ADOP is better by a large margin than all other algorithms (except the M60 tanks scene which has been shot in manual frozen exposure). 
They didn't backport their idea to the comcurrent methods they compare to so I don't think the comparison is really fair (the point based method may simply not be the key element leading to good results).

\noindent  Anyway, their claim to handle the camera imaging pipeline properly is legitimate, extremely well crafted and justified in the paper. But it has a major caveat: results may crumble in case of a more complex or different camera ISP \footnote{Image Signal Processor} than the one from Sony A7SII or DJI X5R.
Please refer to ~\cref{fig:ISP} to compare a standard ISP pipeline against the ADOP rendering.

\noindent The idea is interesting but it will most probably fail with a modern smartphone camera which use sophisticated local correction. Other kind of photo finish effects such as vibrancy make some colors slightly more saturated (e.g. blue for skies but avoid saturating red too much for skin tones), tone mapping may act locally \cite{aubry2014fast} and sharpening may be region dependent. The authors show that they're able to get a sky with cyan shift which looks like the camera picture (they fit the tone mapper correctly). Modern smartphone ISPs manufacturers have worked on this "cyan cast" problem (compress blue highlights instead of clipping when applying white balance) so \textbf{modeling these algorithms will become harder and harder as they get more and more sophisticated.}


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/isp_pipeline_VS_ADOP.png}
    \caption{Comparing a modern ISP camera pipeline at the top versus the ADOP pipeline at the bottom which includes some learnable parameters such as the tone mapping curve, exposure compensation or white balance adjustments.}
    \label{fig:ISP}
\end{figure*}

\noindent Ablation study from the authors (table 3 and figure 6 of \cite{Aruckert2022adop}) shows that ADOP results get better when including the camera module in the pipeline.

\noindent If we carefully look at the Tanks and Temples dataset \cite{Knapitsch2017TanksAndTemples}, pictures are frames extracted from a mp4 video from 2 different high end cameras, sometimes using auto-exposure (see ~\cref{fig:tank_and_temples}). Although it's a good initiative for researchers to keep on using fixed established benchmarks, Tanks and Temples was initially designed for 3D reconstructions (e.g. evaluate performances of Structure from motion like COLMAP estimation compared to a groundtruth lidar captured point cloud) rather than novel view synthesis. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/tanks_and_temples.png}
    \caption{Information on the scenes captured for the Tanks and Temples dataset with high end video cameras in addition to a Lidar point cloud. On the left, the "train" scene shows clear signs of overexposure. The playground scene on the right has an overall correct and steady exposure.}
    \label{fig:tank_and_temples}
\end{figure}


\noindent My recommendation is that all evaluations better be led in the linear domain. It is the best way to make fair comparisons between novel view synthesis algorithms. Benchmarking would strongly benefit from a dataset "upgrade": a sort of \textit{"Linear Tanks and Temples"} carefully crafted by taking RAW shots with a high end DSLR. This is proposed next.

\noindent \textbf{Switching to RAW format?} 
One of the potential way of creating a new benchmark would be to capture the scenes with a DSLR (like a full frame sensor) shot both in RAW and jpg (usually available on most cameras). RAW files would be post-processed by Adobe LightRoom or DxO Photolab with a neutral rendering: no tone mapping and neutral color rendering to get a Linear RGB set of images without any image compression. In case the sensor 12 or 14bits dynamic range is not sufficient, HDR captures could be achieved by bracketing. Intuitively, it sounds natural to use a tripod and merge the LDR linear images into a HDR linear image. These HDR images may serve as the ground truth for validation.
\noindent But even handheld, all raw captures with bracketing / at various exposures could be used (let the alignment of bracketing images be implicitly done by the 3D rendering engine, without any explicit need to merge exposures).

The main caveat of using RAW images is that there'll always be noise present (proper to the camera sensor) in the raw files. Since you get multiple views of the same scene, we can use the key concept proposed in Noise2Noise \cite{lehtinen2018noise2noise} that a neural network can be trained to denoise images on noisy images directly (without clean ground truth images - it requires noisy burst of the same image instead). Here we have access to the same scene under different angles. The engine to implicity align them and get the right supervision is the rendering of the point cloud itself. The idea of using NERF to work on RAW data has been proposed in "NERFs in the dark" \cite{mildenhall2022rawnerf}.


\noindent RGB linear format is the only space in which an evaluation of the rendering quality does not depend on the camera ISP. Supervision could be linear RGB (denoised and demosaicked RAW files)... or simply RAW files (see the pioneer idea in Mosaic2Mosaic \cite{ehret2019joint}).


\subsection{Improvement ideas}
\label{subsec:improvement_ideas}
A lot of my implementation work has been based on trying to apply the concepts of the ADOP paper to calibrated scenes. I think that it is possible to make these photorealistic images look more like the ones we'd get from real cameras.

\subsubsection*{Calibrated scenes + realistic simulated camera pipeline}.
\noindent Regarding my simplified implementation, I did not try to include the camera pipeline module not only because of limited time but also because simulating a realistic camera pipeline is a very difficult task.
Starting from calibrated scenes, here are the steps to mimick a realistic camera pipeline:
\begin{itemize}
    \item output HDR linear images out of Blender \footnote{\texttt{BlenderProc} does not even offer EXR outputs at the moment}.
    \item mimick RAW files: inverse the "blue linear" blocks of ~\cref*{fig:ISP}. The idea of reversing the ISP has been proposed in \cite{brooks2018unprocessing}: apply inverse white balance and color matrices from typical values, mosaick, add realistic poisson+gaussian noise depending on sensor characteristics and ISO value...
    \item Mimick the ISP which goes from 12/14bits linear RAW bayer data to a 8 bit jpg (which is a difficult task). One could simply use a high end software raw converter by re-introducing a DNG as input to Darktable (open source) or Adobe LightRoom. Another way is to use a deep neural network proxy of a blackbox ISP (see the work \cite{ignatov2020replacing} which mimicks the Huawei P20 ISP)
\end{itemize}
This is definitely \textbf{a large chunk of work} but it could bring a lot of value if one wants to try to make a commercial product for novel view synthesis which tries to adapt to all sorts of cameras.

\subsubsection*{Pseudo colors intialization}.
Instead of initializing the pseudo colors of each point randomly, a trick could be to pre-train an auto-encoder on the reference images. For each point in the point cloud projected onto a given view, we have a target patch in the reference view. This patch could simply be encoded into a latent vector which could be aggreagated in the pseudo-color vector. Pushing the idea further, the pretrained decoder (from the auto-encoder) coud be used as the decoder in the neural rendering network. Here's the intuition: if the neural render encoder is the identity, this is equivalent to copy pasting the decoded patches at the right locations of projected points. Fine tuning the network will finish the job of stitching the patches together.

\subsubsection*{Inherent limitations to model view dependant material appearance}.
By construction, the ADOP pipeline does not have a natural ability to model view dependent effects such as specularities or reflections. We have observed this in ~\cref{fig:view_dependancy_issue}.
We could provide extra inputs to the neural network to model view dependent materials. By providing the angle between the point normal and the view direction (these 2 vectors are already computed during the normal culling test in the first hard depth test pass). These 2 angles could be concatenated to the projected pseudo colors vectors (and we'd probably need to map these to sine positional embeddings as proposed in NERF  \cite{mildenhall2020nerf}). \textit{I eventually found out about NPBG++ \cite{rakhimov2022npbg}}. The authors of NPBG++ propose to fit the coefficients of spherical harmonics function which model color variation of each point with regard to relative camera operation.

\subsubsection*{Limitation when zooming too much}.
The inherent limitation of point appears when you zoom in too close. A continuous parametric representation such as an anisotropic 2D Gaussian Kernel would allow mixing the advantage of point clouds (scale and speed) while trying to remove their discrete nature... Gaussian splatting \cite{kerbl3Dgaussians} seems to be one of the most appropriate answer to this problem  and removes the use of a CNN.