\section{Introduction}
\label{sec:intro}
The purpose of this report is a review of the paper ADOP: Approximate Differentiable One-Pixel Point Rendering by \citet{ruckert2022adop}. 
Since the NERF paper published at ECCV 2020, there's been an incredible number of papers on neural rendering . Different approaches have been proposed with an underlying 3D data structure which allows rendering novel views of a scene. Neural radiance fields use a volumetric representation but other families of methods use a "proxy" such as a point clouds \cite{Aliev2020} or even meshes \cite{worchel2022nds}. \\
Let's put things simply: Point based rendering leads to images filled with holes and at first sight does not really look like an appropriate data structure to render continuous surfaces of objects.
We'll see how ADOP manages to use a point cloud structure jointly with an CNN (processing in the image space) to sample dense novel views of large real scenes.

A re-implementation from scratch in Pytorch of some of the key elements of the paper has been made in order to understand the most important points of the ADOP paper. To simplify the study, it seemed like a good idea to work on calibrated synthetic scenes. This way, we can focus on trying to evaluate the relevance of point based rendering and avoid the difficulties inherent to working with real world scenes, most nottably:
\begin{itemize}
    \item We assume linear RGB cameras without tone mappings.
    \item We discard environment map (e.g. our background is black).
    \item We generate photorealistic renders of synthetic meshes. 
    \item Camera poses are perfectly known.
    \item Using meshes allows us sampling point clouds with normals without estimation errors such as the one we'd face with COLMAP.
    \item We can easily control the number of points to be able to tests on limited capacity GPU.
\end{itemize}

\noindent Our code is available on ~\href{https://github.com/balthazarneveu/per-pixel-point-rendering}{GitHub}.
