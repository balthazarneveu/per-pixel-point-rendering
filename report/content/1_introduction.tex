\section{Introduction}
\label{sec:intro}
The purpose of this report is a review of the paper ADOP: Approximate Differentiable One-Pixel Point Rendering by \citet{ruckert2022adop}. 
Since the NERF paper published at ECCV 2020, there's been an incredible number of papers on neural rendering . Different approaches have been proposed with an underlying 3D data structure which allows rendering novel views of a scene. Neural radiance fields use a volumetric representation but other families of methods on a "proxy" such as a point clouds \cite{Aliev2020} or even meshes \cite{worchel2022nds}.
Let's put things simply: Point based rendering leads to images filled with holes and at first sight does not really look like an appropriate data structure to render continuous volumes.
We'll see how ADOP manages to use a point cloud structure jointly with an CNN (processing in the image space) to sample dense novel views of large real scenes.
A re-implementation from scratch in Pytorch of some of the key elements of the paper has been made in order to understand the most important points of the ADOP paper.
Our code is available on ~\href{https://github.com/balthazarneveu/per-pixel-point-rendering}{GitHub}.
