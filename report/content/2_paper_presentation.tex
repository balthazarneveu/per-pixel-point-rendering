\section{Context}
\label{sec:Context}

\textit{ECCV 2020 : The NERF \cite*{mildenhall2020nerf} paper triggers an increasing interest in the field of novel views synthesis}. Can you render novel viewpoints from a given scene using a neural network instead of running a complex ray tracing engine (in a software such as Blender)? The answer is roughly yes and... it also works on real scenes where there's no knowledge of the underlying scene. Many works try to improve the quality/speed of the rendering. We'll see how, suprisingly, point clouds can be used to render novel views of a scene.


\subsection*{What is novel view synthesis?}
\label{sec:novel_view_synthesis}


Novel view synthesis is a standard computer vision task which consists in generating new viewpoints of a scene after capturing a set of images.
When taking real photographs of a scene by walking around (or using a drone), the exact pose of the camera is not perfectly known. Even if an inertial measurement unit is attached to the camera which allows later to have an estimation of the camera pose, there will be measurement errors (sensor noise, calibration error, sensor fusion errors). \textit{Please not that efforts can be made, such as calibrating camera/IMU misalignment} \cite*{karpenko2011gyrostab}. 
So there's a need for an algorithm to estimate the camera poses from images, regardless of having an external pose estimation intialization.
The traditional pipeline consists in using Structure from Motion (SfM) like the popular COLMAP software \cite{schoenberger2016sfm} to jointly estimate camera trajectory. A side product of running this algorithm is getting a colored 3D point cloud of the scene. 
The second step is to reconstruct a flexible representation of the scene so it can be rendered from new viewpoint. The technical challenge is to find the most suited data structure to represent the scene subject to constraints such as: 
\begin{itemize}
    \item image / 3D structure quality: for cultural heritage applications for instance. 
    \item reconstruction time and memory consumption. Real time for AR/VR applications is a constraint. \textit{For instance, ADOP seems to take advantage of point cloud rendering hardware acceleration available in any computer using OpenGL (not necessarily with the need of a massive NVidia GPU)}. 
    \item preprocessing time: in case users want to recreate their own scenes, they may not have access to powerful GPU 
\end{itemize}
There's another usecase where the camera poses and scenes are perfectly known and controlled by using 3D scene synthesis. This is an easier setup to study novel view synthesis as you can truly evaluate the rendering quality of the algorithm without doubts on the quality of pose estimation (or camera photometry). This setup is sometimes refered as "calibrated scenes". One could say that representing scenes with sophisticated neural rendering is useless when you have the underlying 3D model and Blender available. Nevertheless, novel view synthesis on calibrated scenes is a good framework to test a method before deploying on real scenes.

\subsection*{Representations of the scene}
\label{sec:representations}

If we solely use the colored point cloud, we'll end up with images filled with holes (when we zoom in for instance).
This is where rendering a scene start to get difficult. 
\noindent\textbf{Surface reconstruction.}It is possible to get continuous representations (without holes) by wrapping a surface around the point cloud of an object. For instance assuming we have pre-computed normals, a Signed Distance Function (SDF) defined from point cloud can be evaluated anywhere. The surface of the object is where the SDF is equal to zero. Evaluating IMLS (Implicit Moving Least Square \cite*{kolluri2008IMLS}) on a fixed grid followed by the marching cube algorithm allows to recreate a mesh. All topologies may not be represented correctly (e.g. a hole in the cheese may end up being filled).



\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/material_appearance_commented.png}
    \caption{Colors change with camera orientation, specular materials reflect the light and amplify this effect, the extreme use-case being mirrors.}
    \label{fig:material_changes}
\end{figure}

\noindent\textbf{Meshes.} Creating a mesh from the point cloud will lead to a nice geometric representation which can be rendered using classic rasterization techniques. But shading these triangles is still needed. If all materials are perfect diffusers, applying the textures extracted from the photos to the triangles shall be enough. Unfortunately, this will not work for specular materials as illustrated in figure \ref{fig:material_changes}. Neural deferred shading has been proposed \cite{worchel2022nds} to jointly fit a mesh while optimizing a pixel shader (mimicked by a neural network) of a classic mesh rendering pipeline (geometry processing $\rightarrow$ rasterization $\rightarrow$ \textit{(neural)} shader). Please note that the shading is baked into the scene representation and cannot be changed afterwards (for instance, lighting or materials cannot be changed).
\noindent\textbf{Neural radiance fields.} 

\noindent\textbf{Point clouds.} Although point clouds are not continuous, they are a good representation of the scene and kind of easy to manipulate. NPBG \cite{Aliev2020} (Neural Point-Based Graphics) introduced several important concepts. the idea of filling the holes between projected points by using multi-scale convolutional neural networks.  




\section{Methodology of the Original Paper}
\label{sec:methodo_paper}
\subsection{Novel view}
\label{subsec:Projecting points}

\subsection{Point based rendering}
\label{subsec:Projecting points}
Po


\noindent\textbf{Implementation.} The authors used lib Torch to compile the code. 

